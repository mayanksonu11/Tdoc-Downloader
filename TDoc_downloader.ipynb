{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayanksonu11/Tdoc-Downloader/blob/main/TDoc_downloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbXwPxDLUwzK"
      },
      "outputs": [],
      "source": [
        "from openpyxl import load_workbook\n",
        "import datetime\n",
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "8uaCjhuxn12J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# search the xlsx file in the cur rent directory and load them\n",
        "xlsx_files = [f for f in os.listdir() if f.endswith('.xlsx')]\n",
        "if not xlsx_files:\n",
        "    print(\"No .xlsx files found in the current directory.\")\n",
        "else:\n",
        "    # Load the first .xlsx file\n",
        "    xlsx_file = xlsx_files[0]\n",
        "    print(f\"Loading {xlsx_file}...\")\n",
        "    wb = load_workbook(xlsx_file)\n",
        "    # Check if the file is loaded successfully\n",
        "    if wb:\n",
        "        print(f\"Loaded {xlsx_file} successfully.\")\n",
        "    else:\n",
        "        print(f\"Failed to load {xlsx_file}.\")\n",
        "\n",
        "# Select the sheet\n",
        "sheet = wb['TDoc_List']\n",
        "agenda_item = \"8.5.4\""
      ],
      "metadata": {
        "id": "pc665KSVVCcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fdf51c7-b7f2-4777-b2f4-ed1910513095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading TDoc_List_Meeting_RAN2#128.xlsx...\n",
            "Loaded TDoc_List_Meeting_RAN2#128.xlsx successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CFKR1u3ZpFBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract hyperlinks from the TDoc column based on conditions\n",
        "tdoc_links = {}\n",
        "for row in sheet.iter_rows(min_row=2, max_row=1500):  # Adjust min_row as needed\n",
        "    tdoc_cell = row[0]  # Assuming TDoc column is the first column\n",
        "    agenda_item_cell = row[10]  # Assuming Agenda item is the second column\n",
        "    status_cell = row[13]  # Assuming TDoc Status is the third column\n",
        "    title = row[1] # title cell\n",
        "    # print(row)\n",
        "    # print(status_cell.value)\n",
        "    # Check conditions for Agenda item and TDoc Status\n",
        "    if (\n",
        "        # agenda_item_cell.value == agenda_item and\n",
        "        \"CU\" in title.value and\n",
        "        status_cell.value == \"available\"\n",
        "    ):\n",
        "        tdoc_links[tdoc_cell.value] = tdoc_cell.hyperlink.target\n",
        "\n",
        "print(tdoc_links)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL4azrUUVEtJ",
        "outputId": "b11a0c81-c7f6-4ec3-9aae-7e9a14dee1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'R2-2409616': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2409616.zip', 'R2-2409863': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2409863.zip', 'R2-2409873': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2409873.zip', 'R2-2409886': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2409886.zip', 'R2-2409888': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2409888.zip', 'R2-2409953': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2409953.zip', 'R2-2409988': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2409988.zip', 'R2-2410012': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410012.zip', 'R2-2410021': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410021.zip', 'R2-2410035': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410035.zip', 'R2-2410044': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410044.zip', 'R2-2410064': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410064.zip', 'R2-2410113': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410113.zip', 'R2-2410115': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410115.zip', 'R2-2410118': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410118.zip', 'R2-2410136': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410136.zip', 'R2-2410228': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410228.zip', 'R2-2410230': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410230.zip', 'R2-2410242': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410242.zip', 'R2-2410323': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410323.zip', 'R2-2410382': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410382.zip', 'R2-2410389': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410389.zip', 'R2-2410465': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410465.zip', 'R2-2410466': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410466.zip', 'R2-2410518': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410518.zip', 'R2-2410530': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410530.zip', 'R2-2410544': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410544.zip', 'R2-2410598': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410598.zip', 'R2-2410661': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410661.zip', 'R2-2410690': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410690.zip', 'R2-2410695': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410695.zip', 'R2-2410703': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410703.zip', 'R2-2410742': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410742.zip', 'R2-2410856': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410856.zip', 'R2-2410889': 'https://www.3gpp.org/ftp/TSG_RAN/WG2_RL2/TSGR2_128/Docs/R2-2410889.zip'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate over all the links and download it into a folder\n",
        "\n",
        "# Create a directory to save the downloaded files\n",
        "download_dir = \"TDoc_Downloads\"\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "# Download each file\n",
        "for tdoc_name, link in tdoc_links.items():\n",
        "    # Extract the filename from the link\n",
        "    filename = os.path.join(download_dir, os.path.basename(link))\n",
        "    # Check if the file already exists\n",
        "    if os.path.exists(filename):\n",
        "        print(f\"File {filename} already exists. Skipping download.\")\n",
        "        continue\n",
        "    # Download the file\n",
        "    print(f\"Downloading {tdoc_name} from {link}...\")\n",
        "    response = requests.get(link, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    with open(filename, 'wb') as f:\n",
        "        for data in tqdm(response.iter_content(chunk_size=1024), total=total_size // 1024, unit='KB'):\n",
        "            f.write(data)\n",
        "    print(f\"Downloaded {tdoc_name} to {filename}.\")\n",
        "# Print the downloaded files\n",
        "print(\"Downloaded files:\")\n",
        "for filename in os.listdir(download_dir):\n",
        "    print(filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_hLjjfJVNaq",
        "outputId": "cde1b292-a4ae-401e-d129-6c4e9e902231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File TDoc_Downloads/R2-2409616.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2409863.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2409873.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2409886.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2409888.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2409953.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2409988.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410012.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410021.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410035.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410044.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410064.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410113.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410115.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410118.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410136.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410228.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410230.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410242.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410323.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410382.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410389.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410465.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410466.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410518.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410530.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410544.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410598.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410661.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410690.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410695.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410703.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410742.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410856.zip already exists. Skipping download.\n",
            "File TDoc_Downloads/R2-2410889.zip already exists. Skipping download.\n",
            "Downloaded files:\n",
            "R2-2410856.zip\n",
            "R2-2410136.zip\n",
            "R2-2410382.zip\n",
            "R2-2410703.zip\n",
            "R2-2410021.zip\n",
            "R2-2410389.zip\n",
            "R2-2410012.zip\n",
            "R2-2410115.zip\n",
            "R2-2410742.zip\n",
            "R2-2410530.zip\n",
            "R2-2409953.zip\n",
            "R2-2410690.zip\n",
            "R2-2410518.zip\n",
            "R2-2410044.zip\n",
            "R2-2410661.zip\n",
            "R2-2410544.zip\n",
            "R2-2410064.zip\n",
            "R2-2410889.zip\n",
            "R2-2410230.zip\n",
            "R2-2410323.zip\n",
            "R2-2409888.zip\n",
            "R2-2409616.zip\n",
            "R2-2409988.zip\n",
            "R2-2409886.zip\n",
            "R2-2410465.zip\n",
            "R2-2410598.zip\n",
            "R2-2410242.zip\n",
            "R2-2409873.zip\n",
            "R2-2410228.zip\n",
            "R2-2410695.zip\n",
            "R2-2410113.zip\n",
            "R2-2409863.zip\n",
            "R2-2410035.zip\n",
            "R2-2410118.zip\n",
            "R2-2410466.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate over the downloaded files and extract them into separate folders as per the TDoc name\n",
        "# Create a directory to save the extracted files\n",
        "extract_dir = \"TDoc_Extracts\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "# Extract each file\n",
        "for tdoc_name, link in tdoc_links.items():\n",
        "    # Extract the filename from the link\n",
        "    filename = os.path.join(download_dir, os.path.basename(link))\n",
        "    # Create a directory for the extracted files\n",
        "    extract_subdir = os.path.join(extract_dir, tdoc_name)\n",
        "    os.makedirs(extract_subdir, exist_ok=True)\n",
        "    # Check if the file is a zip file\n",
        "    if filename.endswith('.zip'):\n",
        "        print(f\"Extracting {filename} to {extract_subdir}...\")\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_subdir)\n",
        "        print(f\"Extracted {filename} to {extract_subdir}.\")\n",
        "    else:\n",
        "        print(f\"{filename} is not a zip file. Skipping extraction.\")\n",
        "# Print the extracted files\n",
        "print(\"Extracted files:\")\n",
        "for tdoc_name in os.listdir(extract_dir):\n",
        "    print(f\"Files extracted for {tdoc_name}:\")\n",
        "    for filename in os.listdir(os.path.join(extract_dir, tdoc_name)):\n",
        "        print(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9NxM8ctVTK-",
        "outputId": "a643bd48-8a58-49b7-9d6e-b98a8228382e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting TDoc_Downloads/R2-2409616.zip to TDoc_Extracts/R2-2409616...\n",
            "Extracted TDoc_Downloads/R2-2409616.zip to TDoc_Extracts/R2-2409616.\n",
            "Extracting TDoc_Downloads/R2-2409863.zip to TDoc_Extracts/R2-2409863...\n",
            "Extracted TDoc_Downloads/R2-2409863.zip to TDoc_Extracts/R2-2409863.\n",
            "Extracting TDoc_Downloads/R2-2409873.zip to TDoc_Extracts/R2-2409873...\n",
            "Extracted TDoc_Downloads/R2-2409873.zip to TDoc_Extracts/R2-2409873.\n",
            "Extracting TDoc_Downloads/R2-2409886.zip to TDoc_Extracts/R2-2409886...\n",
            "Extracted TDoc_Downloads/R2-2409886.zip to TDoc_Extracts/R2-2409886.\n",
            "Extracting TDoc_Downloads/R2-2409888.zip to TDoc_Extracts/R2-2409888...\n",
            "Extracted TDoc_Downloads/R2-2409888.zip to TDoc_Extracts/R2-2409888.\n",
            "Extracting TDoc_Downloads/R2-2409953.zip to TDoc_Extracts/R2-2409953...\n",
            "Extracted TDoc_Downloads/R2-2409953.zip to TDoc_Extracts/R2-2409953.\n",
            "Extracting TDoc_Downloads/R2-2409988.zip to TDoc_Extracts/R2-2409988...\n",
            "Extracted TDoc_Downloads/R2-2409988.zip to TDoc_Extracts/R2-2409988.\n",
            "Extracting TDoc_Downloads/R2-2410012.zip to TDoc_Extracts/R2-2410012...\n",
            "Extracted TDoc_Downloads/R2-2410012.zip to TDoc_Extracts/R2-2410012.\n",
            "Extracting TDoc_Downloads/R2-2410021.zip to TDoc_Extracts/R2-2410021...\n",
            "Extracted TDoc_Downloads/R2-2410021.zip to TDoc_Extracts/R2-2410021.\n",
            "Extracting TDoc_Downloads/R2-2410035.zip to TDoc_Extracts/R2-2410035...\n",
            "Extracted TDoc_Downloads/R2-2410035.zip to TDoc_Extracts/R2-2410035.\n",
            "Extracting TDoc_Downloads/R2-2410044.zip to TDoc_Extracts/R2-2410044...\n",
            "Extracted TDoc_Downloads/R2-2410044.zip to TDoc_Extracts/R2-2410044.\n",
            "Extracting TDoc_Downloads/R2-2410064.zip to TDoc_Extracts/R2-2410064...\n",
            "Extracted TDoc_Downloads/R2-2410064.zip to TDoc_Extracts/R2-2410064.\n",
            "Extracting TDoc_Downloads/R2-2410113.zip to TDoc_Extracts/R2-2410113...\n",
            "Extracted TDoc_Downloads/R2-2410113.zip to TDoc_Extracts/R2-2410113.\n",
            "Extracting TDoc_Downloads/R2-2410115.zip to TDoc_Extracts/R2-2410115...\n",
            "Extracted TDoc_Downloads/R2-2410115.zip to TDoc_Extracts/R2-2410115.\n",
            "Extracting TDoc_Downloads/R2-2410118.zip to TDoc_Extracts/R2-2410118...\n",
            "Extracted TDoc_Downloads/R2-2410118.zip to TDoc_Extracts/R2-2410118.\n",
            "Extracting TDoc_Downloads/R2-2410136.zip to TDoc_Extracts/R2-2410136...\n",
            "Extracted TDoc_Downloads/R2-2410136.zip to TDoc_Extracts/R2-2410136.\n",
            "Extracting TDoc_Downloads/R2-2410228.zip to TDoc_Extracts/R2-2410228...\n",
            "Extracted TDoc_Downloads/R2-2410228.zip to TDoc_Extracts/R2-2410228.\n",
            "Extracting TDoc_Downloads/R2-2410230.zip to TDoc_Extracts/R2-2410230...\n",
            "Extracted TDoc_Downloads/R2-2410230.zip to TDoc_Extracts/R2-2410230.\n",
            "Extracting TDoc_Downloads/R2-2410242.zip to TDoc_Extracts/R2-2410242...\n",
            "Extracted TDoc_Downloads/R2-2410242.zip to TDoc_Extracts/R2-2410242.\n",
            "Extracting TDoc_Downloads/R2-2410323.zip to TDoc_Extracts/R2-2410323...\n",
            "Extracted TDoc_Downloads/R2-2410323.zip to TDoc_Extracts/R2-2410323.\n",
            "Extracting TDoc_Downloads/R2-2410382.zip to TDoc_Extracts/R2-2410382...\n",
            "Extracted TDoc_Downloads/R2-2410382.zip to TDoc_Extracts/R2-2410382.\n",
            "Extracting TDoc_Downloads/R2-2410389.zip to TDoc_Extracts/R2-2410389...\n",
            "Extracted TDoc_Downloads/R2-2410389.zip to TDoc_Extracts/R2-2410389.\n",
            "Extracting TDoc_Downloads/R2-2410465.zip to TDoc_Extracts/R2-2410465...\n",
            "Extracted TDoc_Downloads/R2-2410465.zip to TDoc_Extracts/R2-2410465.\n",
            "Extracting TDoc_Downloads/R2-2410466.zip to TDoc_Extracts/R2-2410466...\n",
            "Extracted TDoc_Downloads/R2-2410466.zip to TDoc_Extracts/R2-2410466.\n",
            "Extracting TDoc_Downloads/R2-2410518.zip to TDoc_Extracts/R2-2410518...\n",
            "Extracted TDoc_Downloads/R2-2410518.zip to TDoc_Extracts/R2-2410518.\n",
            "Extracting TDoc_Downloads/R2-2410530.zip to TDoc_Extracts/R2-2410530...\n",
            "Extracted TDoc_Downloads/R2-2410530.zip to TDoc_Extracts/R2-2410530.\n",
            "Extracting TDoc_Downloads/R2-2410544.zip to TDoc_Extracts/R2-2410544...\n",
            "Extracted TDoc_Downloads/R2-2410544.zip to TDoc_Extracts/R2-2410544.\n",
            "Extracting TDoc_Downloads/R2-2410598.zip to TDoc_Extracts/R2-2410598...\n",
            "Extracted TDoc_Downloads/R2-2410598.zip to TDoc_Extracts/R2-2410598.\n",
            "Extracting TDoc_Downloads/R2-2410661.zip to TDoc_Extracts/R2-2410661...\n",
            "Extracted TDoc_Downloads/R2-2410661.zip to TDoc_Extracts/R2-2410661.\n",
            "Extracting TDoc_Downloads/R2-2410690.zip to TDoc_Extracts/R2-2410690...\n",
            "Extracted TDoc_Downloads/R2-2410690.zip to TDoc_Extracts/R2-2410690.\n",
            "Extracting TDoc_Downloads/R2-2410695.zip to TDoc_Extracts/R2-2410695...\n",
            "Extracted TDoc_Downloads/R2-2410695.zip to TDoc_Extracts/R2-2410695.\n",
            "Extracting TDoc_Downloads/R2-2410703.zip to TDoc_Extracts/R2-2410703...\n",
            "Extracted TDoc_Downloads/R2-2410703.zip to TDoc_Extracts/R2-2410703.\n",
            "Extracting TDoc_Downloads/R2-2410742.zip to TDoc_Extracts/R2-2410742...\n",
            "Extracted TDoc_Downloads/R2-2410742.zip to TDoc_Extracts/R2-2410742.\n",
            "Extracting TDoc_Downloads/R2-2410856.zip to TDoc_Extracts/R2-2410856...\n",
            "Extracted TDoc_Downloads/R2-2410856.zip to TDoc_Extracts/R2-2410856.\n",
            "Extracting TDoc_Downloads/R2-2410889.zip to TDoc_Extracts/R2-2410889...\n",
            "Extracted TDoc_Downloads/R2-2410889.zip to TDoc_Extracts/R2-2410889.\n",
            "Extracted files:\n",
            "Files extracted for R2-2410530:\n",
            "R2-2410530_Inter-CU_LTM_Security.docx\n",
            "Files extracted for R2-2410518:\n",
            "R2-2410518 Inter-CU LTM.docx\n",
            "Files extracted for R2-2410228:\n",
            "R2-2410228_Discussion on reference configuration for inter-CU LTM.docx\n",
            "Files extracted for R2-2410323:\n",
            "R2-2410323 Discussion on Inter-CU LTM.docx\n",
            "Files extracted for R2-2409988:\n",
            "R2-2409988 Conditional intra-CU LTM.docx\n",
            "Files extracted for R2-2410230:\n",
            "R2-2410230.docx\n",
            "Files extracted for R2-2410690:\n",
            "R2-2410690 Further discussion on inter-CU LTM.docx\n",
            "Files extracted for R2-2410021:\n",
            "R2-2410021 Discussion on inter-CU LTM.docx\n",
            "Files extracted for R2-2410695:\n",
            "R2-2410695_Discussion on Conditional intra-CU LTM.doc\n",
            "Files extracted for R2-2409873:\n",
            "R2-2409873 - Discussion on open issues for inter-CU LTM.docx\n",
            "Files extracted for R2-2410113:\n",
            "R2-2410113 Discussion on Inter-CU LTM.docx\n",
            "Files extracted for R2-2409888:\n",
            "R2-2409888 Further discussion on supporting intra-CU conditional LTM.doc\n",
            "Files extracted for R2-2410856:\n",
            "R2-2410856.docx\n",
            "Files extracted for R2-2410044:\n",
            "R2-2410044 Discussion on Conditional LTM.docx\n",
            "Files extracted for R2-2410598:\n",
            "__MACOSX\n",
            "R2-2410598 (R19 Mob AI 8.6.2) inter-CU LTM.doc\n",
            "Files extracted for R2-2410661:\n",
            "R2-2410661_eMob_Conditional-LTM.docx\n",
            "Files extracted for R2-2409863:\n",
            "R2-2409863 Discussion on Inter CU LTM.docx\n",
            "Files extracted for R2-2410703:\n",
            "R2-2410703-interCU-LTM.docx\n",
            "Files extracted for R2-2410544:\n",
            "R2-2410544 - Security handling and DC aspects for inter-CU LTM.docx\n",
            "Files extracted for R2-2410889:\n",
            "R2-2410889 Support of Conditional Intra-CU LTM_rev1.docx\n",
            "Files extracted for R2-2410382:\n",
            "R2-2410382_8.6.2 LTM for Inter-CU_v3.docx\n",
            "Files extracted for R2-2410064:\n",
            "R2-2410064_Conditional LTM.docx\n",
            "Files extracted for R2-2410742:\n",
            "__MACOSX\n",
            "R2-2410742.docx\n",
            "Files extracted for R2-2410115:\n",
            "R2-2410115 Discussion on conditional intra-CU LTM.docx\n",
            "Files extracted for R2-2410466:\n",
            "R2-2410466-On remaining inter-CU aspects for LTM.docx\n",
            "Files extracted for R2-2410242:\n",
            "R2-2410242 Discussion on inter-CU LTM.docx\n",
            "Files extracted for R2-2410012:\n",
            "R2-2410012 Further Discussion on inter-CU LTM.docx\n",
            "Files extracted for R2-2410035:\n",
            "R2-2410035_Mob_intra_inter-CU.docx\n",
            "Files extracted for R2-2409953:\n",
            "R2-2409953_Conditional LTM Topics_v0.doc\n",
            "Files extracted for R2-2409886:\n",
            "R2-2409886 Further discussion on remaining issues of inter-CU LTM cell switch.doc\n",
            "Files extracted for R2-2410118:\n",
            "R2-2410118 Leftover issues on Inter-CU LTM.docx\n",
            "Files extracted for R2-2410465:\n",
            "R2-2410465 Discussion_on_conditinal_LTM.docx\n",
            "Files extracted for R2-2410389:\n",
            "R2-2410389_8.6.4 Conditional Intra-CU LTM_v1.docx\n",
            "Files extracted for R2-2409616:\n",
            "R2-2409616 inter-CU LTM.docx\n",
            "Files extracted for R2-2410136:\n",
            "R2-2410136 Discussion on conditional intra-CU LTM.doc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44bb9342",
        "outputId": "fc1b5989-b3de-4bb4-e11c-8405d03978bc"
      },
      "source": [
        "%pip install python-docx transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffb05024"
      },
      "source": [
        "import docx\n",
        "from transformers import pipeline\n",
        "\n",
        "def read_docx(file_path):\n",
        "    \"\"\"Reads text from a .docx file.\n",
        "\n",
        "    Args:\n",
        "        file_path: The path to the .docx file.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the text content of the document.\n",
        "    \"\"\"\n",
        "    doc = docx.Document(file_path)\n",
        "    text = []\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text.append(paragraph.text)\n",
        "    return \"\\n\".join(text)\n",
        "\n",
        "def summarize_text(text):\n",
        "    \"\"\"Summarizes the given text using a pre-trained model.\n",
        "\n",
        "    Args:\n",
        "        text: The input text to summarize.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the summarized text.\n",
        "    \"\"\"\n",
        "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "    # The summarizer has a maximum input size, so we might need to split\n",
        "    # the text if it's too long. For this example, we'll just truncate.\n",
        "    max_input_length = 1024  # Adjust this based on the model\n",
        "    summarized_text = summarizer(text[:max_input_length], max_length=1000, min_length=30, do_sample=False)\n",
        "    return summarized_text[0]['summary_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KkoF9scSjT_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7370ea82",
        "outputId": "e76c485b-e2ac-46dc-cf84-97f2e2dc5ebe"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "# Create a list to store the summaries\n",
        "summaries = []\n",
        "\n",
        "# Iterate over the extracted files\n",
        "for tdoc_name in os.listdir(extract_dir):\n",
        "    tdoc_path = os.path.join(extract_dir, tdoc_name)\n",
        "    if os.path.isdir(tdoc_path):\n",
        "        for filename in os.listdir(tdoc_path):\n",
        "            if filename.endswith('.docx') and not filename.startswith('__MACOSX'):\n",
        "                file_path = os.path.join(tdoc_path, filename)\n",
        "                try:\n",
        "                    document_text = read_docx(file_path)\n",
        "                    print(document_text)\n",
        "                    summary = summarize_text(document_text)\n",
        "                    summaries.append({'TDoc Name': tdoc_name, 'Filename': filename, 'Summary': summary})\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "# Define the output CSV file path\n",
        "csv_file_path = \"tdoc_summaries.csv\"\n",
        "\n",
        "# Write the summaries to a CSV file\n",
        "if summaries:\n",
        "    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=['TDoc Name', 'Filename', 'Summary'])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(summaries)\n",
        "    print(f\"Summaries saved to {csv_file_path}\")\n",
        "else:\n",
        "    print(\"No .docx files found to summarize.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3GPP TSG-RAN WG2 #128                                             \t    R2-2410530                                                              \n",
            "Orlando, FL, USA, November 18 – 22 , 2024                                                 \t     \n",
            "\n",
            "Agenda item:\t   8.6.2\n",
            "Source: \t\tQualcomm Incorporated\n",
            "Title: \tSecurity handling for Inter-CU LTM\n",
            "Document for:\tDiscussion\n",
            "1\tIntroduction\n",
            "RAN2 received a response LS (S3-244316) from SA3 on the questions posed by RAN2 on the security handling for inter-CU LTM.\n",
            "SA WG3 has been considering the security mechanisms proposed for the inter-CU LTM feature. For this feature, SA WG3 want to preserve the principle that when there is a PDCP anchor change, then it is necessary to perform a key change (i.e. for solutions or handovers that do not require a PDCP anchor change, there is no need to provide rekeying).\n",
            "The rekeying method preferred by most companies in SA WG3 is that after every inter-CU LTM cell switch execution that changes the PDCP anchor, the UE is provided with the NCC value via RRC signalling to be used by the UE for key derivation at the next inter-CU LTM cell switch. This solution was preferred as it provides the NCC value using the protected RRC signalling and hence is considered to be secure. SA WG3 understands that, RAN WG2 is aiming to avoid RRC configuration/signalling between cell switches for subsequent LTM procedures. If it is necessary for the RAN WGs to avoid RRC signalling between cell switches, then SA WG3 can analyse further methods like, \n",
            "Security threat of the unprotected MAC CE to deliver the NCC value. \n",
            "Sending NCC value per candidate cell via RRC message during LTM preparation phase.\n",
            "SA WG3 will keep RAN WG2 and RAN WG3 informed as we make further progress on this topic.\n",
            "In this contribution, we discuss the SA3 response and possible way forward in RAN2 on this topic.\n",
            "2\tDiscussion\n",
            "RAN2#123bis has sent an LS to SA3 asking for feedback on the proposed solutions in RAN2 for handling the security keys during inter-CU LTM. SA3 has agreed on a Work Item to investigate the general security aspects for LTM ([2]) and has sent an LS response in [3]. \n",
            "In the response LS, it is stated that the solution preferred by most companies in SA3 is to provide the NCC value via RRC signaling. One justification for this choice was the security protection provided by RRC signaling.\n",
            "Observation 1: SA3 prefers that RRC signaling is used to provide the NCC value at or after after the MAC-CE based cell switch command for inter-CU LTM.\n",
            "The LS mentions that SA3 can analyze other options (which were also mentioned in the RAN2 LS) such as using MAC CE or sending NCC value during LTM preparation phase.\n",
            "Observation 2: Per LS response, delivery of NCC via MAC CE or in advance via RRC will require further analysis in SA3.\n",
            "It is obvious that the transfer of an RRC message and RRC reconfiguration during every inter-CU LTM switch will incur significant overhead and delays to the subsequent inter-gNB LTM. This goes against the main premise of LTM which relies on L1/L2 signaling by avoiding L3 messages during LTM execution and during subsequent LTM procedures. This was also captured in the WID ([1]) objective as follows:\n",
            "Specify support for subsequent LTM mobility procedures aiming to avoid RRC configuration between cell switches as per Rel-18 LTM\n",
            "Observation 3: The Rel-19 Mobility WID states that the RRC configuration between cell switches should be avoided.\n",
            "If RRC signaling is used for LTM during every inter-gNB cell switch, LTM will not provide any gains over L3 handover in terms of signaling efficiency and latency benefits.\n",
            "Observation 4: Using RRC signaling and reconfiguration for every inter-CU LTM switch will incur the same latency and signalling overhead as L3 HO.\n",
            "At this point, RAN2 can go in two directions:\n",
            "Option 1: Use RRC signaling for delivery of NCC but work on options to eliminate such signaling at every subsequent inter-CU LTM HO\n",
            "Option 2: Request SA3 to consider the other options, e.g. using MAC CE\n",
            "Option 2 may not be feasible to complete in Rel-19 and MAC-CE is not secure to provide NCC(as commented by SA3 in the LS as well). If MAC-CE needs to be protected, additional work will be needed and this is not within the WID scopre. It took SA3 three meetings to provide the above LS response. A work in SA3 on these additional options will likely take even more time. Rel-19 should finish in RAN2 by Q3 2025 which means that RAN2 will have four meetings in 2025. SA3 will have three meetings in 2025 before the last meeting of RAN2 for Rel-19 in August 2025. Therefore, it is unlikely that RAN2 and SA3 will have sufficient time to agree on another option for security handling.\n",
            "Observation 5: Rel-19 timeline and the pace of progress in SA3 so far makes it very difficult to work on another option other than RRC signaling.\n",
            "Based on this, we propose to use the RRC signaling as a baseline and work on methods to eliminate delivery of NCC at every inter-CU LTM.\n",
            "Proposal 1: RAN2 agree to use RRC signaling for the delivery of NCC for inter-CU LTM\n",
            "Proposal 2: RAN2 agree to support methods to eliminate delivery of NCC for every inter-CU LTM (and without any AMF impact).\n",
            "One option which can make Proposal 2 feasible is the solution discussed in R2-2408966 (4) which relies on not changing the CU anchor at every LTM switch. Even though the discussion on this option has been suspended due to the pending SA3 response, RAN3 has had several rounds of discussion without any progress. As discussed in the paper, the solution relies on mostly backhaul Xn signaling and can be done transparently over Uu.\n",
            "Observation 6: RAN3 has discussed the solution in [4] of keeping the CU anchor unchanged during LTM for several meetings.\n",
            "Observation 7: The solution in [4] is transparent to Uu signaling and relies on enhancements to backhaul signaling.\n",
            "It is important to note that a NW implementation can send NCC via RRC whenever it chooses. However, as discussed above, this will not be desirable for a deployment to do this at every inter-CU LTM.\n",
            "Observation 8: It will be up to NW implementation when to send NCC via RRC for LTM.\n",
            "Given that the solution in [4] or variations of it can be done in RAN3 with minimal RAN2 involvement, RAN2 can agree to a high level solution and request RAN3 to investigate and specify the necessary signaling.\n",
            "Proposal 3: RAN2 request RAN3 to specify solutions for inter-CU LTM which can keep the current CU for the UE unchanged during LTM to cells belonging to other CUs.\n",
            "RAN2 can also inform SA3 on the above agreements. If this is used as the way-forward, no further work in SA3 is needed.\n",
            "Proposal 4: RAN2 to respond to SA3 on using RRC for delivery of NCC (and thus no further SA3 work being necessary).\n",
            "3\tConclusion\n",
            "In this contribution, we discussed the SA3 response LS on the security handling for inter-CU LTM and propose the following:\n",
            "Observation 1: SA3 prefers that RRC signaling is used to provide the NCC value at or after after the MAC-CE based cell switch command for inter-CU LTM.\n",
            "Observation 2: Per SA3 LS response, delivery of NCC via MAC CE or in advance via RRC will require further analysis in SA3.\n",
            "Observation 3: The Rel-19 Mobility WID states that the RRC configuration between cell switches should be avoided.\n",
            "Observation 4: Using RRC signaling and reconfiguration for every inter-CU LTM switch will incur the same latency and signalling overhead as L3 HO.\n",
            "Observation 5: Rel-19 timeline and the pace of progress in SA3 so far makes it very difficult to work on another option other than RRC signaling.\n",
            "Proposal 1: RAN2 agree to use RRC signaling for the delivery of NCC for inter-CU LTM\n",
            "Proposal 2: RAN2 agree to support methods to eliminate delivery of NCC for every inter-CU LTM (and without any AMF impact).\n",
            "Observation 6: RAN3 has discussed the solution in [4] of keeping the CU anchor unchanged during LTM for several meetings.\n",
            "Observation 7: The solution in [4] is transparent to Uu signaling and relies on enhancements to backhaul signaling.\n",
            "Observation 8: It will be up to NW implementation when to send NCC via RRC for LTM.\n",
            "Proposal 3: RAN2 request RAN3 to specify solutions for inter-CU LTM which can keep the current CU for the UE unchanged during LTM to cells belonging to other CUs.\n",
            "Proposal 4: RAN2 to respond to SA3 on using RRC for delivery of NCC (and thus no further SA3 work being necessary).\n",
            "\n",
            "References\n",
            "[1] RP-241515\tRevised Work Item: NR mobility enhancements Phase 4, Apple, China Telecom\n",
            "[2] S3-242401, New WID on security aspects of NR mobility enhancement, Samsung, Oppo\n",
            "[3] S3-244316, Reply LS on security handling for inter-CU LTM in non-DC cases, Samsung\n",
            "[4] R2-2408966, Inter-gNB LTM, Qualcomm Incorporated\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Your max_length is set to 1000, but your input_length is only 412. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=206)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-276869628.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mdocument_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_docx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                     \u001b[0msummaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'TDoc Name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtdoc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Filename'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Summary'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4112272422.py\u001b[0m in \u001b[0;36msummarize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# the text if it's too long. For this example, we'll just truncate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmax_input_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m  \u001b[0;31m# Adjust this based on the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0msummarized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_input_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msummarized_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \"\"\"\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         if (\n\u001b[1;32m    193\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m             )\n\u001b[1;32m   1457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1463\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2650\u001b[0m             )\n\u001b[1;32m   2651\u001b[0m             \u001b[0;31m# 12. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2652\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2653\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2654\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   4095\u001b[0m             \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"output_hidden_states\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_hidden_states\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4097\u001b[0;31m             \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4099\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1469\u001b[0m                 )\n\u001b[1;32m   1470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1472\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1289\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1113\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1116\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             hidden_states, cross_attn_weights = self.encoder_attn(\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0mkey_value_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mattention_interface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py\u001b[0m in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mis_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}